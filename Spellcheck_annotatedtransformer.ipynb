{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spellcheck_annotatedtransformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpatidar/AnnotatedTransformer/blob/master/Spellcheck_annotatedtransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-oL0ot5IJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mLV2ppQ5K5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UbEr8435P2r",
        "colab_type": "code",
        "outputId": "671624c7-238f-4f9e-d377-b9fb38348da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvVt6rrQ5n0Y",
        "colab_type": "code",
        "outputId": "09e06f2e-7f79-4aee-cb8d-8ee2d8ce42f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "! ls /content/drive/My\\ Drive/Pytorch_spellchec/data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint20.pth  model120.pt  model5.pt\t test_results.gsheet\n",
            "checkpoint.pth\t  model1.pt    test.csv\t\t train.csv\n",
            "model10.pt\t  model20.pt   test_results.csv  valid.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mjJ02bd5voP",
        "colab_type": "code",
        "outputId": "5c6af18c-54af-48ae-e7f1-5df2664a2b73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "from platform import python_version\n",
        "print(\"Python version  - \", python_version())\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Pytorch version - \", torch.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version  -  3.6.8\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Pytorch version -  1.3.0+cu100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui75zeR27kJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from torchtext import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL7cveXn7p64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abmtn0nh7vlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFWB1OMD7xjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZxvS7YI7zoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEBZcEUO71iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2C-zjN473sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaNPn3oY75h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ9hRgdV77lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoiGe4Sv79YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yZkYAhf7_Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qLStIOP8A63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sFcRrRi8DB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtewLGAo8FH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hp8j9AH8IfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuSjJPSF8KbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9eskqi-8MyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcGLzKJH8PwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxBTprhd8R_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.source))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.target) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vXVkdjU8T38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "    \n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S22fsncE8V6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUXgnX8a8YyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if True:\n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    BLANK_WORD = \"<blank>\"\n",
        "    \n",
        "    SRC = data.Field(tokenize=word_tokenize, init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "    TGT = data.Field(tokenize=word_tokenize, init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "    MAX_LEN = 100\n",
        "    train, val, test = data.TabularDataset.splits(path=\"/content/drive/My Drive/Pytorch_spellchec/data\", \n",
        "                                                  train =\"train.csv\",validation=\"valid.csv\",test=\"test.csv\", \n",
        "                                                  format='csv', \n",
        "                                                  fields=[(\"target\",TGT),(\"source\",SRC), (\"status\", None)],\n",
        "                                                  skip_header=True)\n",
        "\n",
        "    MIN_FREQ = 1\n",
        "    SRC.build_vocab(train, min_freq=MIN_FREQ)\n",
        "    TGT.build_vocab(train, min_freq=MIN_FREQ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKnvY6nJ8pKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwuAgV2y84fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.source.transpose(0, 1), batch.target.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ86ofVx9tbx",
        "colab_type": "code",
        "outputId": "a862da02-85b8-411f-9957-f9feaf4e63bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "devices=[0]\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "model.cuda()\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "BATCH_SIZE = 64\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=tdevice,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.source), len(x.source)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=tdevice,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.source), len(x.source)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)\n",
        "test_iter = MyIterator(test, batch_size=BATCH_SIZE, device=tdevice,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.source), len(x.source)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLCgcp759vYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMZNo86q6BO1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return loss.item() * norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCD8bBNp6QN8",
        "colab_type": "code",
        "outputId": "eaf763b4-4a78-45da-98a4-ee4ea38d95fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "EPOCHS = 100\n",
        "if True:\n",
        "    model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model, \n",
        "                  SimpleLossCompute(model.generator, criterion, \n",
        "                                       opt=model_opt))\n",
        "        model.eval()\n",
        "        loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                          model, \n",
        "                          SimpleLossCompute(model.generator, criterion, \n",
        "                           opt=None))\n",
        "        print(\"Loss\", loss.item())\n",
        "        print(\"Completed Epoch\", epoch, \"/\", EPOCHS)\n",
        "\n",
        "        if epoch % 50 == 0:\n",
        "          store_model(model, epoch)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-f6fbd03e3b70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   SimpleLossCompute(model.generator, criterion, \n\u001b[0;32m---> 10\u001b[0;31m                                        opt=model_opt))\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
            "\u001b[0;32m<ipython-input-23-3c250c9d9ec4>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m      8\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m      9\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-07b1b4fc53c8>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, y, norm)\u001b[0m\n\u001b[1;32m     10\u001b[0m         loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n\u001b[1;32m     11\u001b[0m                               y.contiguous().view(-1)) / norm\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fziKAjbHwpKB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_model(model, epoch):\n",
        "  checkpoint = {'model': model,\n",
        "                        'state_dict': model.state_dict()}\n",
        "          #'optimizer' : model.optimizer.state_dict()}\n",
        "  filename = '/content/drive/My Drive/Pytorch_spellchec/data/checkpoint'+str(epoch)+'.pth'\n",
        "  torch.save(checkpoint, filename)\n",
        "  print(\"Saved File: \", filename)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMTKUQT2w7_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "7006785b-5831-418d-a3ef-bdf23550ad4b"
      },
      "source": [
        "store_model(model, 50)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Saved File:  /content/drive/My Drive/Pytorch_spellchec/data/checkpoint50.pth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SytKyA607uXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjkzKqTTKvHG",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6C_AR3lKkiH",
        "colab_type": "code",
        "outputId": "59adf689-0826-49ae-ce24-097d7c590f14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderDecoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Encoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type ModuleList. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type EncoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type MultiHeadedAttention. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Linear. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Dropout. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PositionwiseFeedForward. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type SublayerConnection. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type LayerNorm. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Decoder. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type DecoderLayer. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Sequential. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embeddings. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Embedding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:292: UserWarning: Couldn't retrieve source code for container of type Generator. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjP96-Xw6wUX",
        "colab_type": "code",
        "outputId": "8a09a698-fa2f-4006-9f48-8a98c28f6dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def load_checkpoint(filepath):\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model1 = checkpoint['model']\n",
        "    model1.load_state_dict(checkpoint['state_dict'])\n",
        "    for parameter in model.parameters():\n",
        "        parameter.requires_grad = False\n",
        "\n",
        "    \n",
        "    return model1\n",
        "\n",
        "modelload = load_checkpoint('/content/drive/My Drive/Pytorch_spellchec/data/checkpoint.pth')\n",
        "modelload.eval()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): EncoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): DecoderLayer(\n",
              "        (self_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadedAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionwiseFeedForward(\n",
              "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(1887, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (lut): Embedding(1883, 512)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=512, out_features=1883, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj8fiIVD6yZu",
        "colab_type": "code",
        "outputId": "d20dd5b8-0271-4b5e-a41b-92738a07fdf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        }
      },
      "source": [
        "TOP = 10\n",
        "\n",
        "translations = []\n",
        "for i, batch in enumerate(test_iter):\n",
        "    src = batch.source.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(modelload, src, src_mask, \n",
        "                        max_len=100, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    \n",
        "    print(\"Source: \", end=\"\\t\")\n",
        "    for i in range(1, batch.source.size(0)):\n",
        "        sym = SRC.vocab.itos[batch.source.data[i, 0]]\n",
        "        if sym == \"</s>\": \n",
        "          break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "  \n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.target.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.target.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "  \n",
        "    wordlist = []\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "        wordlist.append(sym)\n",
        "    print()\n",
        "  \n",
        "    translations.append(\" \".join(wordlist).strip())\n",
        "    if True:\n",
        "      if i == TOP:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source: \tKnst afdst <unk> \n",
            "Target:\tKnst afdst <unk> \n",
            "Translation:\tKnst 6.6 afstandbus 5X10X10MM \n",
            "Source: \tGuided drive <unk> <unk> \n",
            "Target:\tGuided drive <unk> <unk> \n",
            "Translation:\tSemi-rotary drive DSM-10-240-P-A-FW drive DSM-10-240-P-A-FW \n",
            "Source: \tBG65x25 PI 24V+SG80 <unk> \n",
            "Target:\tBG65x25 PI 24V+SG80 <unk> \n",
            "Translation:\tBG65x25 PI 24V+SG80 24V+SG80 \n",
            "Source: \tHeavy-duty Hex . plugs <unk> \n",
            "Target:\tHeavy-duty hexagon plugs <unk> \n",
            "Translation:\tHeavy-duty hexagon plugs plugs plugs \n",
            "Source: \tAdjustment ring DIN 2250C <unk> \n",
            "Target:\tAdjustment ring DIN 2250C <unk> \n",
            "Translation:\tAdjustment ring DIN 2250C 12.0mm \n",
            "Source: \tThrust ball Bearring single direction <unk> \n",
            "Target:\tThrust ball bearing single direction <unk> \n",
            "Translation:\tThrust ball bearing double direction with cylindrical bore housing washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer washer Thrust \n",
            "Source: \tCompact seal Eriflon 81-0300 <unk> <unk> \n",
            "Target:\tCompact seal Eriflon 81-0300 <unk> <unk> \n",
            "Translation:\tCompact seal Eriflon 81-0300 L=5,6 \n",
            "Source: \tAnti-vibration mount B NR55 <unk> M6x18 \n",
            "Target:\tAnti-vibration mount B NR55 <unk> M6x18 \n",
            "Translation:\tAnti-vibration mount B NR55 30x20 M6x18 \n",
            "Source: \t<unk> RVS A4 <unk> <unk> kvp \n",
            "Target:\t<unk> RVS A4 <unk> <unk> kvp \n",
            "Translation:\tRVS A4 dreind dreind dreind kvp \n",
            "Source: \tHigh precision angular contact Bearring <unk> ACDGB/P4A \n",
            "Target:\tHigh precision angular contact bearing <unk> ACDGB/P4A \n",
            "Translation:\tHigh temperature conical plug - Shape A - minimum Ø x outer length x outer length ( mm ) \n",
            "Source: \tX-ring FKM 75 black 51414 <unk> <unk> \n",
            "Target:\tX-ring FKM 75 black 51414 <unk> <unk> \n",
            "Translation:\tX-ring FKM 75 black 51414 51414 51414 51414 51414 51414 51414 51414 51414 51414 51414 51414 51414 51414 \n",
            "Source: \tHigh precision angular contact Bearring <unk> <unk> \n",
            "Target:\tHigh precision angular contact bearing <unk> <unk> \n",
            "Translation:\tHigh temperature conical plug - Shape A - minimum Ø x outer length x outer length ( mm ) \n",
            "Source: \tX-ring Nitrile rubber 70 black 36624 15,47x3,53 AS568-208 \n",
            "Target:\tX-ring NBR 70 black 36624 15,47x3,53 AS568-208 \n",
            "Translation:\tX-ring NBR 70 black 559273 36624 \n",
            "Source: \tSingle row cylindrical roller Bearring caged NUP <unk> ECP \n",
            "Target:\tSingle row cylindrical roller bearing caged NUP <unk> ECP \n",
            "Translation:\tSingle row cylindrical roller bearing caged NUP 206 206 206 206 NUP \n",
            "Source: \tOil seal - TYPE-FKM GVP <unk> SS 316 spring \n",
            "Target:\tOil seal FKM GVP <unk> SS 316 spring \n",
            "Translation:\tOil seal NBR/fabric model 154-2 154-2 154-2 154-2 spring \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dydVgpHkBy4c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate_sent(sent): \n",
        "  sent = sent.split()\n",
        "  src = torch.LongTensor([[SRC.vocab.stoi[w] for w in sent]]).to(tdevice)\n",
        "\n",
        "  src = Variable(src)\n",
        "  src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "  out = greedy_decode(modelload, src, src_mask, \n",
        "                      max_len=100, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "  \n",
        "  trans = \"\"\n",
        "  for i in range(1, out.size(1)):\n",
        "      sym = TGT.vocab.itos[out[0, i]]\n",
        "      if sym == \"</s>\": break\n",
        "      trans += sym + \" \"\n",
        "  return trans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5226Vbq4COjZ",
        "colab_type": "code",
        "outputId": "3ee0c22d-37d7-452d-ca7e-e144cbf86f7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Pytorch_spellchec/data/test.csv\")\n",
        "print(\"Number of test records:: \", df.shape)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of test records::  (136, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XasRNSwyEge-",
        "colab_type": "code",
        "outputId": "89b2bd6e-6a10-47ea-86dd-ec6c4992379b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "translations = []\n",
        "for idx, row in df.iterrows():\n",
        "  translation = translate_sent(row['source'])\n",
        "  translations.append(translation)\n",
        "  print(\"Completed\",idx)\n",
        "  \n",
        "df['translations'] = translations\n",
        "\n",
        "df['translationMatch?'] = np.where(df.target == df.translations, 'True', 'False')\n",
        "\n",
        "df.to_csv(\"/content/drive/My Drive/Pytorch_spellchec/data/test_results_100checkpoint.csv\")\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Completed 0\n",
            "Completed 1\n",
            "Completed 2\n",
            "Completed 3\n",
            "Completed 4\n",
            "Completed 5\n",
            "Completed 6\n",
            "Completed 7\n",
            "Completed 8\n",
            "Completed 9\n",
            "Completed 10\n",
            "Completed 11\n",
            "Completed 12\n",
            "Completed 13\n",
            "Completed 14\n",
            "Completed 15\n",
            "Completed 16\n",
            "Completed 17\n",
            "Completed 18\n",
            "Completed 19\n",
            "Completed 20\n",
            "Completed 21\n",
            "Completed 22\n",
            "Completed 23\n",
            "Completed 24\n",
            "Completed 25\n",
            "Completed 26\n",
            "Completed 27\n",
            "Completed 28\n",
            "Completed 29\n",
            "Completed 30\n",
            "Completed 31\n",
            "Completed 32\n",
            "Completed 33\n",
            "Completed 34\n",
            "Completed 35\n",
            "Completed 36\n",
            "Completed 37\n",
            "Completed 38\n",
            "Completed 39\n",
            "Completed 40\n",
            "Completed 41\n",
            "Completed 42\n",
            "Completed 43\n",
            "Completed 44\n",
            "Completed 45\n",
            "Completed 46\n",
            "Completed 47\n",
            "Completed 48\n",
            "Completed 49\n",
            "Completed 50\n",
            "Completed 51\n",
            "Completed 52\n",
            "Completed 53\n",
            "Completed 54\n",
            "Completed 55\n",
            "Completed 56\n",
            "Completed 57\n",
            "Completed 58\n",
            "Completed 59\n",
            "Completed 60\n",
            "Completed 61\n",
            "Completed 62\n",
            "Completed 63\n",
            "Completed 64\n",
            "Completed 65\n",
            "Completed 66\n",
            "Completed 67\n",
            "Completed 68\n",
            "Completed 69\n",
            "Completed 70\n",
            "Completed 71\n",
            "Completed 72\n",
            "Completed 73\n",
            "Completed 74\n",
            "Completed 75\n",
            "Completed 76\n",
            "Completed 77\n",
            "Completed 78\n",
            "Completed 79\n",
            "Completed 80\n",
            "Completed 81\n",
            "Completed 82\n",
            "Completed 83\n",
            "Completed 84\n",
            "Completed 85\n",
            "Completed 86\n",
            "Completed 87\n",
            "Completed 88\n",
            "Completed 89\n",
            "Completed 90\n",
            "Completed 91\n",
            "Completed 92\n",
            "Completed 93\n",
            "Completed 94\n",
            "Completed 95\n",
            "Completed 96\n",
            "Completed 97\n",
            "Completed 98\n",
            "Completed 99\n",
            "Completed 100\n",
            "Completed 101\n",
            "Completed 102\n",
            "Completed 103\n",
            "Completed 104\n",
            "Completed 105\n",
            "Completed 106\n",
            "Completed 107\n",
            "Completed 108\n",
            "Completed 109\n",
            "Completed 110\n",
            "Completed 111\n",
            "Completed 112\n",
            "Completed 113\n",
            "Completed 114\n",
            "Completed 115\n",
            "Completed 116\n",
            "Completed 117\n",
            "Completed 118\n",
            "Completed 119\n",
            "Completed 120\n",
            "Completed 121\n",
            "Completed 122\n",
            "Completed 123\n",
            "Completed 124\n",
            "Completed 125\n",
            "Completed 126\n",
            "Completed 127\n",
            "Completed 128\n",
            "Completed 129\n",
            "Completed 130\n",
            "Completed 131\n",
            "Completed 132\n",
            "Completed 133\n",
            "Completed 134\n",
            "Completed 135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qBqhyPYIKS0",
        "colab_type": "code",
        "outputId": "cc6cb3ed-481d-4ad9-f563-8eaa26558ed0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                      target  ... translationMatch?\n",
            "0                 Pipe clamp halves 544,5 AL  ...             False\n",
            "1       Sheet PMMA G clear 10000 3050x2030x2  ...             False\n",
            "2                 Pipe clamp halves 4022 PPH  ...             False\n",
            "3  Filter regulator LFRS-1/4-D-MIDI-A 194733  ...             False\n",
            "4         Insulating tube AF/ARMAFLEX AF6-57  ...             False\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxhJEziZLFQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(\"/content/drive/My Drive/Pytorch_spellchec/data/test_results.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciyck4moLOyl",
        "colab_type": "code",
        "outputId": "cbd193fe-0ab6-4d4b-b095-0478e2874d77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "TOP = 10\n",
        "for idx, row in df.iterrows():\n",
        "  print(row['source'])\n",
        "  print(row['target'])\n",
        "  print(row['translations'])\n",
        "  \n",
        "  if True:\n",
        "    if idx == TOP:\n",
        "      break\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pipe clapm halves 544,5 AL\n",
            "Pipe clamp halves 544,5 AL\n",
            "Pipe clamp halves 108A AL \n",
            "Sheet Pmilli-metreA G clear 10000 3050x2030x2\n",
            "Sheet PMMA G clear 10000 3050x2030x2\n",
            "Sheet PMMA G red 12000 3050x2030x3 \n",
            "Pipe clapm halves 4022 PPH\n",
            "Pipe clamp halves 4022 PPH\n",
            "Pipe clamp halves <unk> AL \n",
            "Filter regulator LFRS-1/4-D-MIDI-A 194733\n",
            "Filter regulator LFRS-1/4-D-MIDI-A 194733\n",
            "Filter regulator <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> \n",
            "Insulating tube AF/ARMAFLEX AF6-57\n",
            "Insulating tube AF/ARMAFLEX AF6-57\n",
            "Insulating tube AF/ARMAFLEX <unk> \n",
            "Speedi-Sleeve 99125 31.75x38.1x11.13milli-metre\n",
            "Speedi-Sleeve 99125 31.75x38.1x11.13mm\n",
            "Speedi-Sleeve 99055 14x19.05x9.93mm \n",
            "Accu boorschrfmach.18V.BS18LT-BL Q Comp. 2 x 2,0 Ah Li-Power, ASC 30-36 V x\n",
            "Accu boorschrfmach.18V.BS18LT-BL Q Comp. 2 x 2,0 Ah Li-Power, ASC 30-36 V x\n",
            "2 316L/316L/C/316L 316L/316L/C/316L 2 10 \n",
            "Chip-board scrwe Countersunk head with Pozidriv HECO+ 3.5x50mm Steel Electrolytic zinc plated\n",
            "Chipboard screw Countersunk head with Pozidriv HECO+ 3.5x50mm Steel Electrolytic zinc plated\n",
            "Chipboard screw Countersunk head with Pozidriv HECO+ 4x40mm Steel Electrolytic zinc plated \n",
            "Ball Bearring 6226-M single row double sealed\n",
            "Ball bearing 6226-M single row double sealed\n",
            "Ball bearing 6011-M-C3 single row double sealed \n",
            "Circlips for bores (interior) DIN 472 36milli-metre Spring steel\n",
            "Circlips for bores (interior) DIN 472 36mm Spring steel\n",
            "Circlips for bores ( interior ) DIN 472 31mm Spring steel \n",
            "Sprocket 08B-1 Z=13 finished Bore 28H7++ hardened\n",
            "Sprocket 08B-1 Z=13 finished Bore 28H7++ hardened\n",
            "Sprocket 08B-1 Z=13 finished Bore 16H7++ hardened \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8loxyrOrRiHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}