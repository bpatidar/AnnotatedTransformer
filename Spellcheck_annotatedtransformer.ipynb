{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spellcheck_annotatedtransformer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bpatidar/AnnotatedTransformer/blob/master/Spellcheck_annotatedtransformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6W-oL0ot5IJV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mLV2ppQ5K5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UbEr8435P2r",
        "colab_type": "code",
        "outputId": "933028e9-731f-4d9e-a7aa-70fe5a9bc9d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvVt6rrQ5n0Y",
        "colab_type": "code",
        "outputId": "416207dd-3e85-4972-ba9d-d0303625cd4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "! ls /content/drive/My\\ Drive/Pytorch_spellchec/data"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.csv  train.csv  valid.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mjJ02bd5voP",
        "colab_type": "code",
        "outputId": "ae4441ea-7c9a-4ef1-91cf-74bc689ff674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from platform import python_version\n",
        "print(\"Python version  - \", python_version())\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math, copy, time\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "seaborn.set_context(context=\"talk\")\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"Pytorch version - \", torch.__version__)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version  -  3.6.8\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Pytorch version -  1.3.0+cu100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui75zeR27kJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from torchtext import data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL7cveXn7p64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(self.encode(src, src_mask), src_mask,\n",
        "                            tgt, tgt_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"Define standard linear + softmax generation step.\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Abmtn0nh7vlw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFWB1OMD7xjA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZxvS7YI7zoA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEBZcEUO71iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2C-zjN473sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaNPn3oY75h_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJ9hRgdV77lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoiGe4Sv79YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yZkYAhf7_Hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qLStIOP8A63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sFcRrRi8DB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtewLGAo8FH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Hp8j9AH8IfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0.0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuSjJPSF8KbN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        Generator(d_model, tgt_vocab))\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform(p)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9eskqi-8MyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, trg=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if trg is not None:\n",
        "            self.trg = trg[:, :-1]\n",
        "            self.trg_y = trg[:, 1:]\n",
        "            self.trg_mask = \\\n",
        "                self.make_std_mask(self.trg, pad)\n",
        "            self.ntokens = (self.trg_y != pad).data.sum()\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & Variable(\n",
        "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcGLzKJH8PwE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(data_iter, model, loss_compute):\n",
        "    \"Standard Training and Logging Function\"\n",
        "    start = time.time()\n",
        "    total_tokens = 0\n",
        "    total_loss = 0\n",
        "    tokens = 0\n",
        "    for i, batch in enumerate(data_iter):\n",
        "        out = model.forward(batch.src, batch.trg, \n",
        "                            batch.src_mask, batch.trg_mask)\n",
        "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
        "        total_loss += loss\n",
        "        total_tokens += batch.ntokens\n",
        "        tokens += batch.ntokens\n",
        "        if i % 50 == 1:\n",
        "            elapsed = time.time() - start\n",
        "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
        "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
        "            start = time.time()\n",
        "            tokens = 0\n",
        "    return total_loss / total_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxBTprhd8R_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global max_src_in_batch, max_tgt_in_batch\n",
        "def batch_size_fn(new, count, sofar):\n",
        "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
        "    global max_src_in_batch, max_tgt_in_batch\n",
        "    if count == 1:\n",
        "        max_src_in_batch = 0\n",
        "        max_tgt_in_batch = 0\n",
        "    max_src_in_batch = max(max_src_in_batch,  len(new.source))\n",
        "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.target) + 2)\n",
        "    src_elements = count * max_src_in_batch\n",
        "    tgt_elements = count * max_tgt_in_batch\n",
        "    return max(src_elements, tgt_elements)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vXVkdjU8T38",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NoamOpt:\n",
        "    \"Optim wrapper that implements rate.\"\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        \"Update parameters and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        \"Implement `lrate` above\"\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * \\\n",
        "            (self.model_size ** (-0.5) *\n",
        "            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "        \n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S22fsncE8V6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(size_average=False)\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUXgnX8a8YyE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if True:\n",
        "    BOS_WORD = '<s>'\n",
        "    EOS_WORD = '</s>'\n",
        "    BLANK_WORD = \"<blank>\"\n",
        "    \n",
        "    SRC = data.Field(tokenize=word_tokenize, pad_token=BLANK_WORD)\n",
        "    TGT = data.Field(tokenize=word_tokenize, init_token = BOS_WORD, eos_token = EOS_WORD, pad_token=BLANK_WORD)\n",
        "\n",
        "    MAX_LEN = 100\n",
        "    train, val, test = data.TabularDataset.splits(path=\"/content/drive/My Drive/Pytorch_spellchec/data\", \n",
        "                                                  train =\"train.csv\",validation=\"valid.csv\",test=\"test.csv\", \n",
        "                                                  format='csv', \n",
        "                                                  fields=[(\"target\",TGT),(\"source\",SRC), (\"status\", None)],\n",
        "                                                  skip_header=True)\n",
        "\n",
        "    MIN_FREQ = 1\n",
        "    SRC.build_vocab(train, min_freq=MIN_FREQ)\n",
        "    TGT.build_vocab(train, min_freq=MIN_FREQ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKnvY6nJ8pKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwuAgV2y84fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyIterator(data.Iterator):\n",
        "    def create_batches(self):\n",
        "        if self.train:\n",
        "            def pool(d, random_shuffler):\n",
        "                for p in data.batch(d, self.batch_size * 100):\n",
        "                    p_batch = data.batch(\n",
        "                        sorted(p, key=self.sort_key),\n",
        "                        self.batch_size, self.batch_size_fn)\n",
        "                    for b in random_shuffler(list(p_batch)):\n",
        "                        yield b\n",
        "            self.batches = pool(self.data(), self.random_shuffler)\n",
        "            \n",
        "        else:\n",
        "            self.batches = []\n",
        "            for b in data.batch(self.data(), self.batch_size,\n",
        "                                          self.batch_size_fn):\n",
        "                self.batches.append(sorted(b, key=self.sort_key))\n",
        "\n",
        "def rebatch(pad_idx, batch):\n",
        "    \"Fix order in torchtext to match ours\"\n",
        "    src, trg = batch.source.transpose(0, 1), batch.target.transpose(0, 1)\n",
        "    return Batch(src, trg, pad_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ86ofVx9tbx",
        "colab_type": "code",
        "outputId": "739a9c29-c2bd-4ce7-abe1-66fc79c8e926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "devices=[0]\n",
        "pad_idx = TGT.vocab.stoi[\"<blank>\"]\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab), N=6)\n",
        "model.cuda()\n",
        "criterion = LabelSmoothing(size=len(TGT.vocab), padding_idx=pad_idx, smoothing=0.1)\n",
        "criterion.cuda()\n",
        "BATCH_SIZE = 64\n",
        "train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=tdevice,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.source), len(x.target)),\n",
        "                            batch_size_fn=batch_size_fn, train=True)\n",
        "valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=tdevice,\n",
        "                            repeat=False, sort_key=lambda x: (len(x.source), len(x.source)),\n",
        "                            batch_size_fn=batch_size_fn, train=False)\n",
        "\n",
        "model_par = nn.DataParallel(model, device_ids=devices)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLCgcp759vYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_opt = NoamOpt(model.src_embed[0].d_model, 1, 2000,\n",
        "            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhcY1l4a9xvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, opt=None):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), \n",
        "                              y.contiguous().view(-1)) / norm\n",
        "        loss.backward()\n",
        "        if self.opt is not None:\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return loss.item() * norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOX6E6NwGrcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Skip if not interested in multigpu.\n",
        "class MultiGPULossCompute:\n",
        "    \"A multi-gpu loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n",
        "        # Send out to different gpus.\n",
        "        self.generator = generator\n",
        "        self.criterion = nn.parallel.replicate(criterion, \n",
        "                                               devices=devices)\n",
        "        self.opt = opt\n",
        "        self.devices = devices\n",
        "        self.chunk_size = chunk_size\n",
        "        \n",
        "    def __call__(self, out, targets, normalize):\n",
        "        total = 0.0\n",
        "        generator = nn.parallel.replicate(self.generator, \n",
        "                                                devices=self.devices)\n",
        "        out_scatter = nn.parallel.scatter(out, \n",
        "                                          target_gpus=self.devices)\n",
        "        out_grad = [[] for _ in out_scatter]\n",
        "        targets = nn.parallel.scatter(targets, \n",
        "                                      target_gpus=self.devices)\n",
        "\n",
        "        # Divide generating into chunks.\n",
        "        chunk_size = self.chunk_size\n",
        "        for i in range(0, out_scatter[0].size(1), chunk_size):\n",
        "            # Predict distributions\n",
        "            out_column = [[Variable(o[:, i:i+chunk_size].data, \n",
        "                                    requires_grad=self.opt is not None)] \n",
        "                           for o in out_scatter]\n",
        "            gen = nn.parallel.parallel_apply(generator, out_column)\n",
        "\n",
        "            # Compute loss. \n",
        "            y = [(g.contiguous().view(-1, g.size(-1)), \n",
        "                  t[:, i:i+chunk_size].contiguous().view(-1)) \n",
        "                 for g, t in zip(gen, targets)]\n",
        "            loss = nn.parallel.parallel_apply(self.criterion, y)\n",
        "\n",
        "            # Sum and normalize loss\n",
        "            l = nn.parallel.gather(loss, \n",
        "                                   target_device=self.devices[0])\n",
        "            l = l.sum().item() / normalize\n",
        "            total += l.item()\n",
        "\n",
        "            # Backprop loss to output of transformer\n",
        "            if self.opt is not None:\n",
        "                l.backward()\n",
        "                for j, l in enumerate(loss):\n",
        "                    out_grad[j].append(out_column[j][0].grad.data.clone())\n",
        "\n",
        "        # Backprop all loss through transformer.            \n",
        "        if self.opt is not None:\n",
        "            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n",
        "            o1 = out\n",
        "            o2 = nn.parallel.gather(out_grad, \n",
        "                                    target_device=self.devices[0])\n",
        "            o1.backward(gradient=o2)\n",
        "            self.opt.step()\n",
        "            self.opt.optimizer.zero_grad()\n",
        "        return total * normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pINhLLrnGveR",
        "colab_type": "code",
        "outputId": "bd314aff-aac5-469d-cb80-5dd1c15dc340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        }
      },
      "source": [
        "devices = [0]\n",
        "for epoch in range(1):\n",
        "      model.train()\n",
        "      run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model, \n",
        "                  MultiGPULossCompute(model.generator, criterion, \n",
        "                                      devices=[tdevice], opt=model_opt))\n",
        "      model.eval()\n",
        "      loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                          model, \n",
        "                          MultiGPULossCompute(model.generator, criterion, \n",
        "                          devices=[tdevice], opt=None))\n",
        "      print(loss)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
            "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-93eaedf7774a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                   MultiGPULossCompute(model.generator, criterion, \n\u001b[0;32m----> 7\u001b[0;31m                                       devices=[tdevice], opt=model_opt))\n\u001b[0m\u001b[1;32m      8\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
            "\u001b[0;32m<ipython-input-26-3c250c9d9ec4>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(data_iter, model, loss_compute)\u001b[0m\n\u001b[1;32m      8\u001b[0m         out = model.forward(batch.src, batch.trg, \n\u001b[1;32m      9\u001b[0m                             batch.src_mask, batch.trg_mask)\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_compute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-598690927c03>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, out, targets, normalize)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;31m# Backprop loss to output of transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mout_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_column\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJmni64L90MI",
        "colab_type": "code",
        "outputId": "78650921-9a56-4a1b-d9fb-3582127e857f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 1\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    run_epoch((rebatch(pad_idx, b) for b in train_iter), \n",
        "                  model, \n",
        "                  SimpleLossCompute(model.generator, criterion, \n",
        "                                       opt=model_opt))\n",
        "    model.eval()\n",
        "    loss = run_epoch((rebatch(pad_idx, b) for b in valid_iter), \n",
        "                          model, \n",
        "                          SimpleLossCompute(model.generator, criterion, \n",
        "                        opt=None))\n",
        "    print(loss)\n",
        "    print(\"Epoch ::-> \", str(epoch) + \"/\" + EPOCHS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 1 Loss: 6.813254 Tokens per Sec: 564.933105\n",
            "Epoch Step: 51 Loss: 6.782098 Tokens per Sec: 707.965271\n",
            "Epoch Step: 101 Loss: 6.672093 Tokens per Sec: 696.455505\n",
            "Epoch Step: 151 Loss: 6.738015 Tokens per Sec: 703.073975\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 699.382812\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.717950 Tokens per Sec: 620.661133\n",
            "Epoch Step: 51 Loss: 6.642831 Tokens per Sec: 711.667114\n",
            "Epoch Step: 101 Loss: 6.665549 Tokens per Sec: 685.009705\n",
            "Epoch Step: 151 Loss: 6.828475 Tokens per Sec: 721.949951\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 704.991272\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.663381 Tokens per Sec: 740.371216\n",
            "Epoch Step: 51 Loss: 6.802716 Tokens per Sec: 715.063721\n",
            "Epoch Step: 101 Loss: 6.808585 Tokens per Sec: 709.722534\n",
            "Epoch Step: 151 Loss: 6.675774 Tokens per Sec: 726.267212\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 685.070557\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.774309 Tokens per Sec: 749.491211\n",
            "Epoch Step: 51 Loss: 6.614414 Tokens per Sec: 702.227661\n",
            "Epoch Step: 101 Loss: 6.618451 Tokens per Sec: 673.304443\n",
            "Epoch Step: 151 Loss: 6.687067 Tokens per Sec: 692.492249\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 690.359314\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.877435 Tokens per Sec: 592.117432\n",
            "Epoch Step: 51 Loss: 6.662393 Tokens per Sec: 693.727112\n",
            "Epoch Step: 101 Loss: 6.648595 Tokens per Sec: 691.272034\n",
            "Epoch Step: 151 Loss: 6.584135 Tokens per Sec: 677.197937\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 697.035767\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.780688 Tokens per Sec: 795.754272\n",
            "Epoch Step: 51 Loss: 6.619075 Tokens per Sec: 677.765625\n",
            "Epoch Step: 101 Loss: 6.702621 Tokens per Sec: 698.357788\n",
            "Epoch Step: 151 Loss: 6.687603 Tokens per Sec: 707.672607\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 632.672241\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.827972 Tokens per Sec: 713.870789\n",
            "Epoch Step: 51 Loss: 6.652689 Tokens per Sec: 716.181152\n",
            "Epoch Step: 101 Loss: 6.662603 Tokens per Sec: 716.918335\n",
            "Epoch Step: 151 Loss: 6.815408 Tokens per Sec: 673.313538\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 527.170471\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.639982 Tokens per Sec: 678.758606\n",
            "Epoch Step: 51 Loss: 6.969616 Tokens per Sec: 711.057922\n",
            "Epoch Step: 101 Loss: 6.936388 Tokens per Sec: 669.797058\n",
            "Epoch Step: 151 Loss: 6.513986 Tokens per Sec: 712.723511\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 665.956909\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.634565 Tokens per Sec: 718.590088\n",
            "Epoch Step: 51 Loss: 6.706845 Tokens per Sec: 714.798218\n",
            "Epoch Step: 101 Loss: 6.644875 Tokens per Sec: 707.475220\n",
            "Epoch Step: 151 Loss: 6.616869 Tokens per Sec: 660.233398\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 691.016052\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.718971 Tokens per Sec: 735.798950\n",
            "Epoch Step: 51 Loss: 6.624878 Tokens per Sec: 721.522339\n",
            "Epoch Step: 101 Loss: 6.750944 Tokens per Sec: 710.781799\n",
            "Epoch Step: 151 Loss: 6.559169 Tokens per Sec: 690.526306\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 591.179504\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.599658 Tokens per Sec: 736.242249\n",
            "Epoch Step: 51 Loss: 6.774128 Tokens per Sec: 704.589172\n",
            "Epoch Step: 101 Loss: 6.777844 Tokens per Sec: 713.590759\n",
            "Epoch Step: 151 Loss: 6.676689 Tokens per Sec: 697.075073\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 685.526855\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.761355 Tokens per Sec: 537.713074\n",
            "Epoch Step: 51 Loss: 6.504855 Tokens per Sec: 644.053101\n",
            "Epoch Step: 101 Loss: 6.783952 Tokens per Sec: 693.089233\n",
            "Epoch Step: 151 Loss: 6.577861 Tokens per Sec: 713.188416\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 704.124634\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.483765 Tokens per Sec: 667.488281\n",
            "Epoch Step: 51 Loss: 6.684625 Tokens per Sec: 724.392578\n",
            "Epoch Step: 101 Loss: 6.772970 Tokens per Sec: 700.944824\n",
            "Epoch Step: 151 Loss: 6.870100 Tokens per Sec: 694.202148\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 649.046204\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.703189 Tokens per Sec: 751.384460\n",
            "Epoch Step: 51 Loss: 6.611928 Tokens per Sec: 675.361755\n",
            "Epoch Step: 101 Loss: 6.928913 Tokens per Sec: 718.291077\n",
            "Epoch Step: 151 Loss: 6.627121 Tokens per Sec: 710.485352\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 523.809265\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.740420 Tokens per Sec: 542.303650\n",
            "Epoch Step: 51 Loss: 6.505311 Tokens per Sec: 650.054199\n",
            "Epoch Step: 101 Loss: 6.728314 Tokens per Sec: 734.916199\n",
            "Epoch Step: 151 Loss: 6.729686 Tokens per Sec: 683.362183\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 706.081116\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.597981 Tokens per Sec: 713.588928\n",
            "Epoch Step: 51 Loss: 6.611126 Tokens per Sec: 703.130981\n",
            "Epoch Step: 101 Loss: 6.697749 Tokens per Sec: 715.478638\n",
            "Epoch Step: 151 Loss: 6.602531 Tokens per Sec: 714.109192\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 699.315125\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.758593 Tokens per Sec: 717.837463\n",
            "Epoch Step: 51 Loss: 6.680312 Tokens per Sec: 695.445251\n",
            "Epoch Step: 101 Loss: 6.776702 Tokens per Sec: 672.293640\n",
            "Epoch Step: 151 Loss: 6.551886 Tokens per Sec: 676.503784\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 646.682190\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.694864 Tokens per Sec: 756.483948\n",
            "Epoch Step: 51 Loss: 6.602840 Tokens per Sec: 713.249207\n",
            "Epoch Step: 101 Loss: 6.698016 Tokens per Sec: 699.069702\n",
            "Epoch Step: 151 Loss: 6.706985 Tokens per Sec: 665.855591\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 699.717773\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.688322 Tokens per Sec: 702.487305\n",
            "Epoch Step: 51 Loss: 6.776484 Tokens per Sec: 707.661011\n",
            "Epoch Step: 101 Loss: 6.632090 Tokens per Sec: 712.526855\n",
            "Epoch Step: 151 Loss: 6.572799 Tokens per Sec: 712.479248\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 686.248108\n",
            "tensor(6.7135, device='cuda:0')\n",
            "Epoch Step: 1 Loss: 6.649419 Tokens per Sec: 742.470581\n",
            "Epoch Step: 51 Loss: 6.733111 Tokens per Sec: 698.674561\n",
            "Epoch Step: 101 Loss: 6.566049 Tokens per Sec: 678.285034\n",
            "Epoch Step: 151 Loss: 6.682792 Tokens per Sec: 673.289185\n",
            "Epoch Step: 1 Loss: 6.747511 Tokens per Sec: 649.439575\n",
            "tensor(6.7135, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp5oyjNMGqEr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRX8LpMF92oQ",
        "colab_type": "code",
        "outputId": "72e81a9f-c186-423f-a389-f88c7a31e063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99
        }
      },
      "source": [
        "for i, batch in enumerate(valid_iter):\n",
        "    src = batch.source.transpose(0, 1)[:1]\n",
        "    src_mask = (src != SRC.vocab.stoi[\"<blank>\"]).unsqueeze(-2)\n",
        "    out = greedy_decode(model, src, src_mask, \n",
        "                        max_len=100, start_symbol=TGT.vocab.stoi[\"<s>\"])\n",
        "    print(\"Translation:\", end=\"\\t\")\n",
        "    for i in range(1, out.size(1)):\n",
        "        sym = TGT.vocab.itos[out[0, i]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Target:\", end=\"\\t\")\n",
        "    for i in range(1, batch.target.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.target.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    print(\"Source:\", end=\"\\t\")\n",
        "    for i in range(1, batch.source.size(0)):\n",
        "        sym = TGT.vocab.itos[batch.source.data[i, 0]]\n",
        "        if sym == \"</s>\": break\n",
        "        print(sym, end =\" \")\n",
        "    print()\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translation:\t68x8 68x8 68x8 68x8 68x8 150-450 150-450 150-450 150-450 150-450 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 M10 \n",
            "Target:\tLinear/swivel clamp <unk> <unk> \n",
            "Source:\tclamp <unk> <unk> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUjW_0vgD5Ja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
        "    for i in range(max_len-1):\n",
        "        out = model.decode(memory, src_mask, \n",
        "                           Variable(ys), \n",
        "                           Variable(subsequent_mask(ys.size(1))\n",
        "                                    .type_as(src.data)))\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim = 1)\n",
        "        next_word = next_word.data[0]\n",
        "        ys = torch.cat([ys, \n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
        "    return ys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ_3AIvhE6Gy",
        "colab_type": "code",
        "outputId": "103a77d5-0e89-4053-909f-86170c56b2f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i, batch in enumerate(valid_iter):\n",
        "  print(i, batch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 \n",
            "[torchtext.data.batch.Batch of size 10]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 6x10 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 4x10 (GPU 0)]\n",
            "1 \n",
            "[torchtext.data.batch.Batch of size 10]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 6x10 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 4x10 (GPU 0)]\n",
            "2 \n",
            "[torchtext.data.batch.Batch of size 9]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 7x9 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 5x9 (GPU 0)]\n",
            "3 \n",
            "[torchtext.data.batch.Batch of size 7]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 7x7 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 5x7 (GPU 0)]\n",
            "4 \n",
            "[torchtext.data.batch.Batch of size 7]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 9x7 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 6x7 (GPU 0)]\n",
            "5 \n",
            "[torchtext.data.batch.Batch of size 6]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 10x6 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 6x6 (GPU 0)]\n",
            "6 \n",
            "[torchtext.data.batch.Batch of size 6]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 10x6 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 6x6 (GPU 0)]\n",
            "7 \n",
            "[torchtext.data.batch.Batch of size 7]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 9x7 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 7x7 (GPU 0)]\n",
            "8 \n",
            "[torchtext.data.batch.Batch of size 7]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 9x7 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 7x7 (GPU 0)]\n",
            "9 \n",
            "[torchtext.data.batch.Batch of size 7]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 9x7 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 7x7 (GPU 0)]\n",
            "10 \n",
            "[torchtext.data.batch.Batch of size 6]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 10x6 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 8x6 (GPU 0)]\n",
            "11 \n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 11x5 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 8x5 (GPU 0)]\n",
            "12 \n",
            "[torchtext.data.batch.Batch of size 6]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 10x6 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 8x6 (GPU 0)]\n",
            "13 \n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 10x5 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 9x5 (GPU 0)]\n",
            "14 \n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 12x5 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 9x5 (GPU 0)]\n",
            "15 \n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 12x5 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 10x5 (GPU 0)]\n",
            "16 \n",
            "[torchtext.data.batch.Batch of size 4]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 13x4 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 10x4 (GPU 0)]\n",
            "17 \n",
            "[torchtext.data.batch.Batch of size 5]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 12x5 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 10x5 (GPU 0)]\n",
            "18 \n",
            "[torchtext.data.batch.Batch of size 4]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 13x4 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 11x4 (GPU 0)]\n",
            "19 \n",
            "[torchtext.data.batch.Batch of size 4]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 13x4 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 11x4 (GPU 0)]\n",
            "20 \n",
            "[torchtext.data.batch.Batch of size 3]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 15x3 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 13x3 (GPU 0)]\n",
            "21 \n",
            "[torchtext.data.batch.Batch of size 3]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 19x3 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 17x3 (GPU 0)]\n",
            "22 \n",
            "[torchtext.data.batch.Batch of size 3]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 21x3 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 19x3 (GPU 0)]\n",
            "23 \n",
            "[torchtext.data.batch.Batch of size 1]\n",
            "\t[.target]:[torch.cuda.LongTensor of size 21x1 (GPU 0)]\n",
            "\t[.source]:[torch.cuda.LongTensor of size 19x1 (GPU 0)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "612OO70MFgdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}